{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/welton/project/.env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from optuna.distributions import (IntDistribution,\n",
    "                                    FloatDistribution)\n",
    "from joblib import load, dump\n",
    "\n",
    "from local_utils import load_stacking_probs, build_clf_beans\n",
    "\n",
    "# Configs Optuna\n",
    "import warnings\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "from optuna.logging import set_verbosity, WARNING\n",
    "\n",
    "set_verbosity(WARNING)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"20ng\"]\n",
    "CLFS = [\"kpr\", \"ktr\", \"lpr\", \"ltr\", \"sfr\", \"stmk\", \"xfr\", \"xpr\", \"xtr\", \"kfr\", \"ktmk\", \"lfr\", \"ltmk\", \"spr\", \"str\", \"xlnet_softmax\", \"xtmk\", \"rep_bert\"]\n",
    "#CLFS = [\"kpr\", \"lfr\"]\n",
    "ORACLE_DIR = \"/home/welton/data/oracle\"\n",
    "ESTIMATOR_NAME = \"xgboost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement_mfs(probas, clf_target, fold, train_test):\n",
    "\n",
    "    main_preds = probas[clf_target][fold][train_test].argmax(axis=1)\n",
    "    preds = [ probas[clf][fold][train_test].argmax(axis=1) for clf in probas if clf != clf_target ]\n",
    "    preds = np.vstack(preds).T\n",
    "\n",
    "    div = []\n",
    "    #agree_classes = []\n",
    "    agree_sizes = []\n",
    "    for idx in np.arange(main_preds.shape[0]):\n",
    "        pred_class, agree_size = Counter(preds[idx]).most_common()[0]\n",
    "        if pred_class == main_preds[idx]:\n",
    "            div.append(0)\n",
    "        else:\n",
    "            div.append(1)\n",
    "        #agree_classes.append(pred_class)\n",
    "        agree_sizes.append(agree_size)\n",
    "\n",
    "    #return div, agree_classes, agree_sizes\n",
    "    return np.array(div), np.array(agree_sizes)\n",
    "\n",
    "def confidence_rate(probas, labels):\n",
    "\n",
    "    conf_hit = []\n",
    "    conf_freq, hits = build_clf_beans(probas, labels)\n",
    "    hits_rate = { np.trunc(bean*10)/10 : hits[bean] / conf_freq[bean] if bean in hits else 0 for bean in np.arange(0, 1, 0.1) }\n",
    "    preds = probas.argmax(axis=1)\n",
    "    for idx, predicted_class in enumerate(preds):\n",
    "        # Getting the probability of the predicted class\n",
    "        probability = probas[idx][predicted_class] * 10\n",
    "        bean = np.trunc(probability) / 10\n",
    "        bean = 0.9 if bean >= 1 else bean\n",
    "        conf_hit.append(hits_rate[bean])\n",
    "    return np.array(conf_hit)\n",
    "\n",
    "def hits_rate_by_class(probas, labels):\n",
    "\n",
    "    class_hits_rate = {}\n",
    "    preds = probas.argmax(axis=1)\n",
    "    # Vector with hits and misses.\n",
    "    hits = preds == labels\n",
    "    # For each label.\n",
    "    for label in np.unique(labels):\n",
    "        # Get the docs of the label.\n",
    "        class_docs = labels == label\n",
    "        class_hits_rate[label] = np.sum(hits[class_docs]) / np.sum(class_docs)\n",
    "    return np.array([ class_hits_rate[p] for p in preds ])\n",
    "\n",
    "def class_weights(probas, labels):\n",
    "\n",
    "    cw = { label:np.sum(labels == label) / labels.shape[0] for label in np.unique(labels) }\n",
    "    preds = probas.argmax(axis=1)\n",
    "    return np.array([ cw[p] for p in preds ])\n",
    "\n",
    "def get_clf(clf = \"xgboost\", n_jobs=5):\n",
    "\n",
    "    if clf == \"xgboost\":\n",
    "        CLF_XGB = XGBClassifier(random_state=42, verbosity=0, n_jobs=n_jobs, tree_method=\"gpu_hist\")\n",
    "        HYP_XGB = {\n",
    "            \"n_estimators\": IntDistribution(low=100, high=1000, step=50),\n",
    "            \"learning_rate\": FloatDistribution(low=.01, high=.5),\n",
    "            \"eta\": FloatDistribution(low=.025, high=.5),\n",
    "            \"max_depth\": IntDistribution(low=1, high=14),\n",
    "            \"subsample\": FloatDistribution(low=.5, high=1.),\n",
    "            \"gamma\": FloatDistribution(low=1e-8, high=1.),\n",
    "            \"colsample_bytree\": FloatDistribution(low=.5, high=1.)\n",
    "        }\n",
    "        return CLF_XGB, HYP_XGB\n",
    "    else:\n",
    "        HYP_GBM = {}\n",
    "        return GradientBoostingClassifier(), HYP_GBM\n",
    "\n",
    "def execute_optimization(\n",
    "        classifier_name: str,\n",
    "        file_model: str,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        *,\n",
    "        opt_cv: int = 4,\n",
    "        opt_n_iter: int = 30,\n",
    "        opt_scoring: str = \"f1_macro\",\n",
    "        opt_n_jobs: int = 5,\n",
    "        clf_n_jobs: int = 5,\n",
    "        seed: int = 42,\n",
    "        load_model: bool = False\n",
    ") -> BaseEstimator:\n",
    "\n",
    "    classifier, hyperparameters = get_clf(classifier_name, n_jobs=clf_n_jobs)\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"classifier\", classifier)\n",
    "    ])\n",
    "    hyperparameters = {f\"classifier__{k}\": v for k, v in hyperparameters.items()}\n",
    "\n",
    "    optuna_search = OptunaSearchCV(\n",
    "        pipeline,\n",
    "        hyperparameters,\n",
    "        cv=StratifiedKFold(opt_cv, shuffle=True, random_state=seed),\n",
    "        error_score=\"raise\",\n",
    "        n_trials=opt_n_iter,\n",
    "        random_state=seed,\n",
    "        scoring=opt_scoring,\n",
    "        n_jobs=opt_n_jobs\n",
    "    )\n",
    "\n",
    "    if load_model and os.path.exists(file_model):\n",
    "        print(\"\\tModel already executed! Loading model...\", end=\"\")\n",
    "        optuna_search = load(file_model)\n",
    "    else:\n",
    "        print(\"\\tExecuting model...\", end=\"\")\n",
    "        optuna_search.fit(X_train, y_train)\n",
    "        dump(optuna_search, file_model)\n",
    "\n",
    "    return optuna_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DATASETS[0]\n",
    "probas = load_stacking_probs(dataset, CLFS, \"train_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "# For each fold.\n",
    "for fold in np.arange(10):\n",
    "    # Loading labels.\n",
    "    y_train = np.load(f\"/home/welton/data/datasets/labels/split_10/{dataset}/{fold}/train.npy\")\n",
    "    y_test = np.load(f\"/home/welton/data/datasets/labels/split_10/{dataset}/{fold}/test.npy\")\n",
    "    # Load fold Meta-Features (Washington).\n",
    "    dist_train = csr_matrix(np.load(f\"/home/welton/data/meta_features/features/dist/{fold}/{dataset}/train.npz\")[\"X_train\"]).toarray()\n",
    "    dist_test = csr_matrix(np.load(f\"/home/welton/data/meta_features/features/dist/{fold}/{dataset}/test.npz\")[\"X_test\"]).toarray()\n",
    "    # For each Stacking base model.\n",
    "    for target_clf in [\"lfr\"]:\n",
    "\n",
    "        # Building Meta-Features.\n",
    "        probas_train = probas[target_clf][fold][\"train\"]\n",
    "        probas_test = probas[target_clf][fold][\"test\"]\n",
    "        cw_train = class_weights(probas_train, y_train)\n",
    "        cw_test = class_weights(probas_test, y_test)\n",
    "        hrc_train = hits_rate_by_class(probas_train, y_train)\n",
    "        hrc_test = hits_rate_by_class(probas_test, y_test)\n",
    "        conf_train = confidence_rate(probas_train, y_train)\n",
    "        conf_test = confidence_rate(probas_test, y_test)\n",
    "        div_train, ags_train = agreement_mfs(probas, target_clf, fold, \"train\")\n",
    "        div_test, ags_test = agreement_mfs(probas, target_clf, fold, \"test\")\n",
    "        scaled_ags_train = MinMaxScaler().fit_transform(ags_train.reshape(-1, 1)).reshape(-1)\n",
    "        scaled_ags_test = MinMaxScaler().fit_transform(ags_test.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Joining Meta-Features.\n",
    "        X_train = np.vstack([\n",
    "            cw_train,\n",
    "            hrc_train,\n",
    "            conf_train,\n",
    "            div_train,\n",
    "            ags_train,\n",
    "            scaled_ags_train\n",
    "        ]).T\n",
    "\n",
    "        X_train = np.hstack([probas_train, dist_train, X_train])\n",
    "        X_test = np.vstack([\n",
    "            cw_test,\n",
    "            hrc_test,\n",
    "            conf_test,\n",
    "            div_test,\n",
    "            ags_test,\n",
    "            scaled_ags_test\n",
    "        ]).T\n",
    "        X_test = np.hstack([probas_test, dist_test, X_test])\n",
    "\n",
    "        \n",
    "        # Making labels (hit or missed)\n",
    "        preds_train = probas_train.argmax(axis=1)\n",
    "        upper_train = np.zeros(preds_train.shape[0])\n",
    "        upper_train[preds_train == y_train] = 1\n",
    "\n",
    "        preds_test = probas_test.argmax(axis=1)\n",
    "        upper_test = np.zeros(preds_test.shape[0])\n",
    "        upper_test[preds_test == y_test] = 1\n",
    "\n",
    "        output_dir = f\"{ORACLE_DIR}/{dataset}/{target_clf}/{fold}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        file_model = f\"{output_dir}/model\"\n",
    "\n",
    "        \"\"\"\n",
    "        error_estimator = XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.11,\n",
    "                max_depth=11,\n",
    "                booster=\"gbtree\",\n",
    "                colsample_bytree=0.650026359170959,\n",
    "                random_state=42,\n",
    "                verbosity=0,\n",
    "                n_jobs=1,\n",
    "                tree_method='gpu_hist')\n",
    "\n",
    "        error_estimator.fit(X_train[:1000], upper_train[:1000])\n",
    "\n",
    "        # Prediction\n",
    "        y_pred = error_estimator.predict(X_test)\n",
    "        \n",
    "        scores.append([target_clf,\n",
    "        precision_score(upper_test, y_pred, pos_label=0),\n",
    "        recall_score(upper_test, y_pred, pos_label=0),\n",
    "        f1_score(upper_test, y_pred, pos_label=0),\n",
    "        accuracy_score(upper_test, y_pred)])\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16970, 466), (1876, 466), (16970,), (1876,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, upper_train.shape, upper_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16956, 20), (1890, 20))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas[\"stmk\"][2]['train'].shape, probas[\"stmk\"][2]['test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.11,\n",
    "        max_depth=11,\n",
    "        booster=\"gbtree\",\n",
    "        colsample_bytree=0.650026359170959,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=1,\n",
    "        tree_method='gpu_hist')\n",
    "\n",
    "    # Training xgb.\n",
    "    _ = xgb.fit(X_train, y_train)\n",
    "    # Evaluating xgb's performance.\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    return f1\n",
    "\n",
    "def feature_selection(X_train, X_test, y_train, y_test):\n",
    "    # Training RF and building feature importance ranking.\n",
    "    forest = RandomForestClassifier(n_estimators=300, max_depth=11, random_state=42, n_jobs=25)\n",
    "    _ = forest.fit(X_train, y_train)\n",
    "    importances = forest.feature_importances_\n",
    "    ranking = (1 - importances).argsort()\n",
    "\n",
    "    pick = 5\n",
    "    gap = 10\n",
    "    best_f1 = -1\n",
    "    best_pos = -1\n",
    "    # se é para subir a busca ou descer.\n",
    "    improve = True\n",
    "\n",
    "    # Enquanto houver pontos para busca.\n",
    "    while True:\n",
    "        # Teste o modelo com 'pick' features (Aqui pode ser o XGBoost na GPU).\n",
    "        feats_ids = ranking[:pick]\n",
    "        f1 = get_f1(X_train[:, feats_ids], X_test[:, feats_ids], y_train, y_test)\n",
    "        # Se a macro de agora for melhor que a última.\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            best_pos = pick\n",
    "            improve = True\n",
    "        # Se a macro de agora não for melhor que a última encurte o salto.\n",
    "        else:\n",
    "            improve = False\n",
    "            gap = max(gap // 2, 1)\n",
    "            improve = False\n",
    "        pick = best_pos + gap\n",
    "        \n",
    "        if gap == 1 and not improve:\n",
    "            break\n",
    "\n",
    "    gap = 10\n",
    "    improve = True\n",
    "    pick = best_pos - gap + 1\n",
    "    first_best_pos = best_pos\n",
    "    # Enquanto houver pontos para busca.\n",
    "    while pick < first_best_pos:\n",
    "        # Teste o modelo com 'pick' features (Aqui pode ser o XGBoost na GPU).\n",
    "        feats_ids = ranking[:pick]\n",
    "        f1 = get_f1(X_train[:, feats_ids.tolist()], X_test[:, feats_ids.tolist()], y_train, y_test)\n",
    "        # Se a macro de agora for melhor que a última.\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            best_pos = pick\n",
    "            improve = True\n",
    "        # Se a macro de agora não for melhor que a última encurte o salto.\n",
    "        else:\n",
    "            improve = False\n",
    "            gap = gap // 2\n",
    "            improve = False\n",
    "        \n",
    "        pick = first_best_pos - gap\n",
    "    print(f\"F1: {best_f1} POS: {best_pos}\")\n",
    "\n",
    "    return best_pos, forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8230554411911337\n",
      "F1: 0.831950603343813\n",
      "F1: 0.8353124483545731\n",
      "F1: 0.83206159154604\n",
      "F1: 0.8303922209867708\n",
      "F1: 0.8322875521993938\n",
      "F1: 0.8300752996144113\n",
      "F1: 0.8336074921228812\n",
      "F1: 0.8358549506847928\n",
      "F1: 0.8358549506847928\n",
      "F1: 0.8358121437377386\n",
      "F1: 0.8358549506847928 POS: 23\n"
     ]
    }
   ],
   "source": [
    "best_pos, forest = feature_selection(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kpr;26,59974905897114;97,6958525345622;41,81459566074951;28,311057108140947\n",
      "lfr;22,595419847328245;78,3068783068783;35,07109004739336;33,41433778857837\n",
      "kpr;26,879699248120303;65,0;38,03191489361702;43,30900243309003\n",
      "lfr;22,85276073619632;75,25252525252525;35,05882352941177;32,846715328467155\n",
      "kpr;26,7741935483871;71,55172413793103;38,96713615023475;36,662606577344704\n",
      "lfr;23,06477093206951;70,53140096618358;34,761904761904766;33,252131546894034\n",
      "kpr;24,27652733118971;69,26605504587155;35,95238095238095;34,47015834348356\n",
      "lfr;23,625557206537888;85,94594594594595;37,06293706293706;34,22655298416565\n",
      "kpr;29,97448979591837;94,75806451612904;45,54263565891474;31,5468940316687\n",
      "lfr;22,755417956656345;76,5625;35,08353221957041;33,73934226552984\n",
      "kpr;21,875;49,763033175355446;30,39073806078148;41,34146341463414\n",
      "lfr;24,085365853658537;82,29166666666666;37,264150943396224;35,12195121951219\n",
      "kpr;27,95969773299748;99,10714285714286;43,614931237721024;29,914529914529915\n",
      "lfr;23,23076923076923;79,47368421052632;35,95238095238095;34,31013431013431\n",
      "kpr;30,77922077922078;97,53086419753086;46,791707798617956;34,10757946210269\n",
      "lfr;25,150602409638555;83,91959798994975;38,702201622247976;35,33007334963325\n",
      "kpr;27,509293680297397;68,83720930232559;39,309428950863214;44,063647490820074\n",
      "lfr;20,0;81,48148148148148;32,11678832116788;31,701346389228885\n",
      "kpr;28,050314465408803;99,55357142857143;43,76840039254171;29,865361077111384\n",
      "lfr;25,0;80,09950248756219;38,10650887573965;35,98531211750306\n"
     ]
    }
   ],
   "source": [
    "for s in scores:\n",
    "    print(f\"{s[0]};{s[1]*100};{s[2]*100};{s[3]*100};{s[4]*100}\".replace('.', ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.array([\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5]\n",
    "])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4],\n",
       "       [2, 4],\n",
       "       [2, 4],\n",
       "       [2, 4],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, [1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3135864b85fc339145ed731976b9bcbd775d49eba38ad5ba8da470ad91643c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
