{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"../analysis/utils/\")\n",
    "\n",
    "from utils import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"webkb\", \"20ng\", \"acm\", \"reut\"]\n",
    "\n",
    "CLFS = [\"rep_bert\"]\n",
    "CLFS = [\"kpr\", \"ktr\", \"lpr\", \"ltr\", \"sfr\", \"stmk\", \"xfr\", \"xpr\", \"xtr\", \"kfr\", \"ktmk\", \"lfr\", \"ltmk\", \"spr\", \"str\", \"xlnet_softmax\", \"xtmk\", \"rep_bert\"]\n",
    "\n",
    "THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_datasets = get_datasets(DATASETS, path=\"../../data/pd_datasets/__dset__.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf_beans(clf_probas, label):\n",
    "    predictions = clf_probas.argmax(axis=1)\n",
    "    confidence_freq = {}\n",
    "    hits = {}\n",
    "    # For each prediction\n",
    "    for idx, predicted_class in enumerate(predictions):\n",
    "        \n",
    "        # Getting the probability of the predicted class\n",
    "        probability = clf_probas[idx][predicted_class] * 10\n",
    "        bean = np.trunc(probability) / 10\n",
    "        bean = 0.9 if bean >= 1 else bean\n",
    "        # Adding the bean in confidence if is not there yet.\n",
    "        if bean not in confidence_freq:\n",
    "            confidence_freq[bean] = 0\n",
    "        confidence_freq[bean] += 1\n",
    "        # Veryfing if the predicted class was right.\n",
    "        if predicted_class == label[idx]:\n",
    "            if bean not in hits:\n",
    "                hits[bean] = 0\n",
    "            hits[bean] += 1\n",
    "    return confidence_freq, hits\n",
    "\n",
    "def get_miss_predictor(confidence_freq, hits, threshold=0.3):\n",
    "\n",
    "    predictor = {}\n",
    "    # For each confidence interval.\n",
    "    for bean in hits:\n",
    "        # Get the hit rate.\n",
    "        hits_rate = hits[bean] / confidence_freq[bean]\n",
    "        \n",
    "        if hits_rate < threshold:\n",
    "            predictor[bean] = True\n",
    "    return predictor\n",
    "\n",
    "def predict(X, estimator):\n",
    "    \n",
    "    estimates = []\n",
    "    predictions = X.argmax(axis=1)\n",
    "    # For each prediction.\n",
    "    for idx, predicted_class in enumerate(predictions):\n",
    "        probability = X[idx][predicted_class] * 10\n",
    "        bean = np.trunc(probability) / 10\n",
    "        bean = 0.9 if bean >= 1 else bean\n",
    "        # If this confidence has a miss rate greater than THRESHOLD (wether it is in the dictionary or not)\n",
    "        if bean in estimator:\n",
    "            estimates.append(0)\n",
    "        else:\n",
    "            estimates.append(1)\n",
    "    return np.array(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for dset in DATASETS:\n",
    "    print(f\"{dset.upper()}\")\n",
    "    for clf in CLFS:\n",
    "        print(f\"\\t{clf.upper()}\")\n",
    "        for fold in np.arange(10):\n",
    "            probs_dir = f\"/home/welton/data/clfs_output/split_10/{dset}/10_folds/{clf}/{fold}\"\n",
    "            # Loading probabilities.\n",
    "            X_train = np.load(f\"{probs_dir}/train.npz\")[\"X_train\"]\n",
    "            labels_dir = f\"/home/welton/data/datasets/labels/split_10/{dset}/{fold}\"\n",
    "            train_labels = np.load(f\"{labels_dir}/train.npy\")\n",
    "            \n",
    "            X_test = np.load(f\"{probs_dir}/test.npz\")[\"X_test\"]\n",
    "            \n",
    "            # Building error estimator.\n",
    "            confidence_freq, hits = build_clf_beans(X_train, train_labels)\n",
    "            estimator = get_miss_predictor(confidence_freq, hits, THRESHOLD)\n",
    "            \n",
    "            # Applying estimator on train and test.\n",
    "            train_est = predict(X_train, estimator)\n",
    "            test_est = predict(X_test, estimator)\n",
    "            \n",
    "            ## Saving the new probabilities (features for the meta-layer).\n",
    "            #output_dir = f\"/home/welton/data/oracle/hits_rate/{THRESHOLD}/{dset}/{clf}/{fold}\"\n",
    "            #os.makedirs(output_dir, exist_ok=True)\n",
    "            #np.savez(f\"{output_dir}/test\", y=test_est)\n",
    "            #np.savez(f\"{output_dir}/train\", y=train_est)\n",
    "\n",
    "            # Saving probabilities to hits_rate_test\n",
    "            output_dir = f\"/home/welton/data/oracle/hits_rate_test/{THRESHOLD}/{dset}/{clf}/{fold}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            train_ground_truth = np.load(f\"/home/welton/data/oracle/upper_bound/{dset}/{clf}/{fold}/train.npz\")[\"y\"]\n",
    "            np.savez(f\"{output_dir}/test\", y=test_est)\n",
    "            np.savez(f\"{output_dir}/train\", y=train_ground_truth)\n",
    "\n",
    "            ## Comparing this strategy with \n",
    "            #y_true = np.load(f\"/home/welton/data/oracle/upper_bound/{dset}/{clf}/{fold}/test.npz\")[\"y\"]\n",
    "            #prec = np.round(precision_score(y_true, test_est, zero_division=1, pos_label=0) * 100, decimals=2)\n",
    "            #rec = np.round(recall_score(y_true, test_est, pos_label=0) * 100, decimals=2)\n",
    "            #print(f\"\\t\\tFOLD: {fold} - Precision: {prec} Recall: {rec}\")\n",
    "            #scores.append([dset, clf, fold, prec, rec])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scores, columns=[\"DATASET\", \"CLF\", \"Precision\", \"Recall\", \"Fold\"])\n",
    "df.to_excel(f\"data/{THRESHOLD}.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.load(\"../../data/oracle/upper_bound/webkb/lfr/0/test.npz\").keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(np.load(\"../../data/oracle/hits_rate/0.1/20ng/xlnet_softmax/0/test.npz\")['y'], return_counts=True),\n",
    "np.unique(np.load(\"../../data/oracle/hits_rate/0.1/20ng/xlnet_softmax/0/train.npz\")['y'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(np.load(\"../../data/oracle/hits_rate/0.2/20ng/xlnet_softmax/0/test.npz\")['y'], return_counts=True),\n",
    "np.unique(np.load(\"../../data/oracle/hits_rate/0.2/20ng/xlnet_softmax/0/train.npz\")['y'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(np.load(\"../../data/oracle/hits_rate/0.3/20ng/xlnet_softmax/0/test.npz\")['y'], return_counts=True),\n",
    "np.unique(np.load(\"../../data/oracle/hits_rate/0.3/20ng/xlnet_softmax/0/train.npz\")['y'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3135864b85fc339145ed731976b9bcbd775d49eba38ad5ba8da470ad91643c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
