{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"../analysis/utils/\")\n",
    "\n",
    "from utils import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"webkb\", \"20ng\"]\n",
    "\n",
    "CLFS = [\"rep_bert\"]\n",
    "CLFS = [\"kpr\", \"ktr\", \"lpr\", \"ltr\", \"sfr\", \"stmk\", \"xfr\", \"xpr\", \"xtr\", \"kfr\", \"ktmk\", \"lfr\", \"ltmk\", \"spr\", \"str\", \"xlnet_softmax\", \"xtmk\", \"rep_bert\"]\n",
    "\n",
    "THRESHOLD = [0.1, 0.3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_datasets = get_datasets(DATASETS, path=\"../../data/pd_datasets/__dset__.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf_beans(clf_probas, label):\n",
    "    predictions = clf_probas.argmax(axis=1)\n",
    "    confidence_freq = {}\n",
    "    hits = {}\n",
    "    # For each prediction\n",
    "    for idx, predicted_class in enumerate(predictions):\n",
    "        \n",
    "        # Getting the probability of the predicted class\n",
    "        probability = clf_probas[idx][predicted_class] * 10\n",
    "        bean = np.trunc(probability) / 10\n",
    "        bean = 0.9 if bean >= 1 else bean\n",
    "        # Adding the bean in confidence if is not there yet.\n",
    "        if bean not in confidence_freq:\n",
    "            confidence_freq[bean] = 0\n",
    "        confidence_freq[bean] += 1\n",
    "        # Veryfing if the predicted class was right.\n",
    "        if predicted_class == label[idx]:\n",
    "            if bean not in hits:\n",
    "                hits[bean] = 0\n",
    "            hits[bean] += 1\n",
    "    return confidence_freq, hits\n",
    "\n",
    "def get_miss_predictor(confidence_freq, hits, threshold=0.3):\n",
    "\n",
    "    predictor = {}\n",
    "    # For each confidence interval.\n",
    "    for bean in hits:\n",
    "        # Get the hit rate.\n",
    "        hits_rate = hits[bean] / confidence_freq[bean]\n",
    "        \n",
    "        if hits_rate < threshold:\n",
    "            predictor[bean] = True\n",
    "    return predictor\n",
    "\n",
    "def predict(X, estimator):\n",
    "    \n",
    "    estimates = []\n",
    "    predictions = X.argmax(axis=1)\n",
    "    # For each prediction.\n",
    "    for idx, predicted_class in enumerate(predictions):\n",
    "        probability = X[idx][predicted_class] * 10\n",
    "        bean = np.trunc(probability) / 10\n",
    "        bean = 0.9 if bean >= 1 else bean\n",
    "        # If this confidence has a miss rate greater than THRESHOLD (wether it is in the dictionary or not)\n",
    "        if bean in estimator:\n",
    "            estimates.append(0)\n",
    "        else:\n",
    "            estimates.append(1)\n",
    "    return np.array(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7376, 823]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m np\u001b[39m.\u001b[39msavez(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m/train\u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m=\u001b[39mnormal)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Comparing this strategy with \u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m prec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(precision_score(y_true, test_est, zero_division\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, pos_label\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, decimals\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     40\u001b[0m rec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(recall_score(y_true, test_est, pos_label\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, decimals\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39m#print(f\"\\t\\tFOLD: {fold} - Precision: {prec} Recall: {rec}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/project/.env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1776\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[1;32m   1648\u001b[0m     y_true,\n\u001b[1;32m   1649\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1656\u001b[0m ):\n\u001b[1;32m   1657\u001b[0m     \u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \n\u001b[1;32m   1659\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1776\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1777\u001b[0m         y_true,\n\u001b[1;32m   1778\u001b[0m         y_pred,\n\u001b[1;32m   1779\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1780\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1781\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1782\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1783\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1784\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1786\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/project/.env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1563\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1562\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1563\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/project/.env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1364\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1362\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1364\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1365\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/project/.env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/project/.env/lib/python3.8/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7376, 823]"
     ]
    }
   ],
   "source": [
    "for thr in THRESHOLD:\n",
    "    scores = []\n",
    "    for dset in DATASETS:\n",
    "        #print(f\"{dset.upper()}\")\n",
    "        for clf in CLFS:\n",
    "            #print(f\"\\t{clf.upper()}\")\n",
    "            for fold in np.arange(10):\n",
    "                probs_dir = f\"/home/welton/data/clfs_output/split_10/{dset}/10_folds/{clf}/{fold}\"\n",
    "                # Loading probabilities.\n",
    "                X_train = np.load(f\"{probs_dir}/train.npz\")[\"X_train\"]\n",
    "                labels_dir = f\"/home/welton/data/datasets/labels/split_10/{dset}/{fold}\"\n",
    "                train_labels = np.load(f\"{labels_dir}/train.npy\")\n",
    "                \n",
    "                X_test = np.load(f\"{probs_dir}/test.npz\")[\"X_test\"]\n",
    "                \n",
    "                # Building error estimator.\n",
    "                confidence_freq, hits = build_clf_beans(X_train, train_labels)\n",
    "                estimator = get_miss_predictor(confidence_freq, hits, thr)\n",
    "                \n",
    "                # Applying estimator on train and test.\n",
    "                train_est = predict(X_train, estimator)\n",
    "                test_est = predict(X_test, estimator)\n",
    "                \n",
    "                ## Saving the new probabilities (features for the meta-layer).\n",
    "                #output_dir = f\"/home/welton/data/oracle/hits_rate/{thr}/{dset}/{clf}/{fold}\"\n",
    "                #os.makedirs(output_dir, exist_ok=True)\n",
    "                #np.savez(f\"{output_dir}/test\", y=test_est)\n",
    "                #np.savez(f\"{output_dir}/train\", y=train_est)\n",
    "\n",
    "                # Saving probabilities to hits_rate_test\n",
    "                output_dir = f\"/home/welton/data/oracle/hits_rate_test/{thr}/{dset}/{clf}/{fold}\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                y_true = np.load(f\"/home/welton/data/oracle/upper_bound/{dset}/{clf}/{fold}/train.npz\")[\"y\"]\n",
    "                normal = np.zeros(y_true.shape[0]) + 1\n",
    "                np.savez(f\"{output_dir}/test\", y=test_est)\n",
    "                np.savez(f\"{output_dir}/train\", y=normal)\n",
    "\n",
    "                # Comparing this strategy with \n",
    "                prec = np.round(precision_score(y_true, test_est, zero_division=1, pos_label=0) * 100, decimals=2)\n",
    "                rec = np.round(recall_score(y_true, test_est, pos_label=0) * 100, decimals=2)\n",
    "                #print(f\"\\t\\tFOLD: {fold} - Precision: {prec} Recall: {rec}\")\n",
    "                scores.append([dset, clf, prec, rec, fold])\n",
    "                df = pd.DataFrame(scores, columns=[\"DATASET\", \"CLF\", \"Precision\", \"Recall\", \"Fold\"])\n",
    "                df.to_excel(f\"data/{thr}.xlsx\", index=False)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.load(\"../../data/oracle/upper_bound/webkb/lfr/0/test.npz\").keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(np.load(\"../../data/oracle/hits_rate/0.1/20ng/xlnet_softmax/0/test.npz\")['y'], return_counts=True),\n",
    "np.unique(np.load(\"../../data/oracle/hits_rate/0.1/20ng/xlnet_softmax/0/train.npz\")['y'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(np.load(\"../../data/oracle/hits_rate/0.2/20ng/xlnet_softmax/0/test.npz\")['y'], return_counts=True),\n",
    "np.unique(np.load(\"../../data/oracle/hits_rate/0.2/20ng/xlnet_softmax/0/train.npz\")['y'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.unique(np.load(\"../../data/oracle/hits_rate/0.3/20ng/xlnet_softmax/0/test.npz\")['y'], return_counts=True),\n",
    "np.unique(np.load(\"../../data/oracle/hits_rate/0.3/20ng/xlnet_softmax/0/train.npz\")['y'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3135864b85fc339145ed731976b9bcbd775d49eba38ad5ba8da470ad91643c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
