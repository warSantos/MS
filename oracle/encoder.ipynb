{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/welton/project/.env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Tuple, List\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DatasetCent(Dataset):\n",
    "\n",
    "\t# Data: Pass X and Y as a tuple i.e., data = (X, Y)\n",
    "\tdef __init__(self, data):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tx = data[0]\n",
    "\t\ty = data[1]\n",
    "\n",
    "\t\tself.x = torch.from_numpy(x)\n",
    "\t\tself.y = torch.from_numpy(y)\n",
    "\t\tself.n_samples = x.shape[0]\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x[index], self.y[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_samples\n",
    "\n",
    "class EconderArchitecture(nn.Module):\n",
    "\n",
    "\tdef __init__(self, input_size, hidden_layers=3, output_layer=\"linear\"):\n",
    "\t\t\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\t\tself.encoder = nn.ModuleList()\n",
    "\t\t\n",
    "\t\tfor i in range(hidden_layers):\n",
    "\t\t\tself.encoder.append(nn.Linear(self.input_size, self.output_size))\n",
    "\n",
    "\t\tif output_layer == \"tanh\":\n",
    "\t\t\tself.encoder.append(nn.Tanh())\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor layer in self.encoder:\n",
    "\t\t\tx = layer(x)\n",
    "\t\treturn x\n",
    "\n",
    "class Encoder:\n",
    "\n",
    "\tdef __init__(self, input_size, hidden_layers=3):\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.device = torch.device(\n",
    "\t\t\t'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\t\tself.model = EconderArchitecture(\n",
    "\t\t\tself.input_size, hidden_layers=hidden_layers).to(self.device)\n",
    "\n",
    "\tdef fit(self, data: DatasetCent, epochs=30, batch_size=16):\n",
    "\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "\t\ttrain_loader = DataLoader(\n",
    "\t\t\tdataset=data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\t\t# Train the model\n",
    "\t\tn_total_steps = len(train_loader)\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tfor i, (x, y) in enumerate(train_loader):\n",
    "\n",
    "\t\t\t\tx = x.to(self.device)\n",
    "\t\t\t\ty = y.to(self.device)\n",
    "\n",
    "\t\t\t\t# Forward pass\n",
    "\t\t\t\toutputs = self.model(x.float())\n",
    "\t\t\t\tloss = criterion(outputs.float(), y.float())\n",
    "\n",
    "\t\t\t\t# Backward and optimize\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\tif (i+1) % 100 == 0:\n",
    "\t\t\t\t\tprint(\n",
    "\t\t\t\t\t\tf'\\t\\tEpoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}', end=\"\\r\")\n",
    "\t\tprint(\"\\n\")\n",
    "\n",
    "\tdef predict(self, X: torch.Tensor):\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tX_pred = []\n",
    "\t\t\tfor x in X.to(self.device):\n",
    "\t\t\t\tpred = self.model(x.float())\n",
    "\t\t\t\tX_pred.append(pred)\n",
    "\n",
    "\t\treturn np.array(torch.stack(X_pred).cpu().numpy())\n",
    "\n",
    "\n",
    "class EncoderMFs:\n",
    "\n",
    "\tdef transform(self, X_train, y_train, X_test, params):\n",
    "\n",
    "\t\tdata_loader = DatasetCent((X_train, y_train))\n",
    "\t\tenc = Encoder(X_train.shape[1], params[\"hidden_layers\"])\n",
    "\t\tenc.fit(data_loader)\n",
    "\t\ttry:\n",
    "\t\t\treturn enc.predict(torch.from_numpy(X_test.todense()))\n",
    "\t\texcept:\n",
    "\t\t\treturn enc.predict(torch.from_numpy(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_y(\n",
    "        file: str,\n",
    "        test_train: str\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    loaded = np.load(file, allow_pickle=True)\n",
    "    X = loaded[f\"X_{test_train}\"]\n",
    "    \n",
    "    if f\"y_{test_train}\" not in loaded:\n",
    "        return X, None\n",
    "\n",
    "    y = loaded[f\"y_{test_train}\"]\n",
    "\n",
    "    if X.size == 1:\n",
    "        X = X.item()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def read_train_test_meta_oracle(\n",
    "        dir_meta_input: str,\n",
    "        dataset: str,\n",
    "        n_folds: int,\n",
    "        fold_id: int,\n",
    "        algorithms: List[str],\n",
    "        oracle_path: str,\n",
    "        oracle_strategy: str\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    Xs_train, Xs_test = [], []\n",
    "\n",
    "    for alg in algorithms:\n",
    "        file_train_meta = f\"{dir_meta_input}/{dataset}/{n_folds}_folds/{alg}/{fold_id}/train.npz\"\n",
    "        file_test_meta = f\"{dir_meta_input}/{dataset}/{n_folds}_folds/{alg}/{fold_id}/test.npz\"\n",
    "\n",
    "        X_train_meta, _ = load_x_y(file_train_meta, 'train')\n",
    "        X_test_meta, _ = load_x_y(file_test_meta, 'test')\n",
    "\n",
    "        if oracle_strategy in [\"upper_bound\", \"upper_test\", \"upper_train\"]:\n",
    "            oracle_base_dir = f\"{oracle_path}/{oracle_strategy}/{dataset}/{alg}/{fold_id}\"\n",
    "            oracle_file_train = f\"{oracle_base_dir}/train.npz\"\n",
    "            oracle_file_test = f\"{oracle_base_dir}/test.npz\"\n",
    "\n",
    "            oracle_train = np.load(oracle_file_train)['y']\n",
    "            oracle_test = np.load(oracle_file_test)['y']\n",
    "\n",
    "            Xs_train.append(X_train_meta * oracle_train[:, None])\n",
    "            Xs_test.append(X_test_meta * oracle_test[:, None])\n",
    "        else:\n",
    "            Xs_train.append(X_train_meta)\n",
    "            Xs_test.append(X_test_meta)\n",
    "\n",
    "    X_train_meta = np.hstack(Xs_train)\n",
    "    X_test_meta = np.hstack(Xs_test)\n",
    "\n",
    "    return X_train_meta, X_test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLFS = [\"kpr\", \"ktr\", \"lpr\", \"ltr\", \"sfr\", \"stmk\", \"xfr\", \"xpr\", \"xtr\", \"kfr\", \"ktmk\", \"lfr\", \"ltmk\", \"spr\", \"str\", \"xlnet_softmax\", \"xtmk\", \"rep_bert\"]\n",
    "DATA_SOURCE = \"/home/welton/data\"\n",
    "META_INPUT = f\"{DATA_SOURCE}/clfs_output/split_10\"\n",
    "UPPER_INPUT = f\"{DATA_SOURCE}/oracle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = read_train_test_meta_oracle(META_INPUT, \"webkb\", 10, 0, CLFS, UPPER_INPUT, \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OX_train, OX_test = read_train_test_meta_oracle(META_INPUT, \"webkb\", 10, 0, CLFS, UPPER_INPUT, \"upper_bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DatasetCent((X_train, OX_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tEpoch [30/30], Step [400/461], Loss: 0.0173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.fit(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = encoder.predict(torch.from_numpy(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.917242166236974"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred[0] - OX_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.999999995470716"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_test[0] - OX_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3135864b85fc339145ed731976b9bcbd775d49eba38ad5ba8da470ad91643c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
