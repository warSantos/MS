{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-02-16T19:20:28.128568Z","iopub.status.busy":"2023-02-16T19:20:28.127933Z","iopub.status.idle":"2023-02-16T19:20:38.134710Z","shell.execute_reply":"2023-02-16T19:20:38.133756Z","shell.execute_reply.started":"2023-02-16T19:20:28.128535Z"},"trusted":true},"outputs":[],"source":["# imports\n","import io\n","import os\n","import pickle\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","\n","import torch\n","from torch.nn.functional import softmax\n","from torch.utils import data \n","from transformers import AutoTokenizer\n","from datasets import load_metric\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","from transformers import AutoModelForSequenceClassification\n","from transformers import get_scheduler\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-16T19:20:38.137908Z","iopub.status.busy":"2023-02-16T19:20:38.136983Z","iopub.status.idle":"2023-02-16T19:20:38.205616Z","shell.execute_reply":"2023-02-16T19:20:38.203346Z","shell.execute_reply.started":"2023-02-16T19:20:38.137866Z"},"trusted":true},"outputs":[],"source":["def get_doc_by_id(X, idxs):\n","\n","    docs = []\n","    for idx in idxs:\n","        docs.append(X[idx])\n","    return docs\n","\n","\n","def data_process(data_dir, dataset, fold):\n","\n","    split_settings = pd.read_pickle(\n","        f\"{data_dir}/{dataset}/split_10_with_val.pkl\")\n","    \n","    with io.open(f\"{data_dir}/{dataset}/texts.txt\", newline='\\n', errors='ignore') as read:\n","        X = []\n","        for row in read:\n","            X.append(row.strip())\n","\n","    labels = []\n","    with open(f\"{data_dir}/{dataset}/score.txt\") as fd:\n","        for line in fd:\n","            labels.append(int(line.strip()))\n","    labels = np.array(labels)\n","    num_labels = len(np.unique(labels))\n","\n","    # For binary classification if there is a -1 label, replace it for 0.\n","    labels[labels == -1] = 0\n","    # If the min class valeu is 1, subtract 1 from every label.\n","    if np.min(labels == 1):\n","        labels = labels - 1\n","\n","    sp_set = split_settings[split_settings.fold_id == fold]\n","\n","    X_train = get_doc_by_id(X, sp_set.train_idxs.tolist()[0])\n","    X_test = get_doc_by_id(X, sp_set.test_idxs.tolist()[0])\n","    X_val = get_doc_by_id(X, sp_set.val_idxs.tolist()[0])\n","\n","    y_train = labels[sp_set.train_idxs.iloc[0]]\n","    y_test = labels[sp_set.test_idxs.iloc[0]]\n","    y_val = labels[sp_set.val_idxs.iloc[0]]\n","\n","    return X_train, X_test, X_val, y_train, y_test, y_val, split_settings, num_labels\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) \n","          for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","class Transformer():\n","\n","    def __init__(\n","        self,\n","        batch_size=16,\n","        limit_k=20,\n","        max_epochs=5,\n","        lr=5e-5,\n","        max_length=256,\n","        limit_patient=3,\n","        model_name=\"bert-base-cased\",\n","        padding=True,\n","        truncation=True,\n","        seed=42,\n","        load_model: bool = True\n","    ):\n","        self.batch_size = batch_size\n","        self.limit_k = limit_k\n","        self.max_epochs = max_epochs\n","        self.lr = lr\n","        self.max_length = max_length\n","        self.limit_patient = limit_patient\n","        self.model_name = model_name\n","        self.padding = padding\n","        self.truncation = truncation\n","        self.seed = seed\n","        self.load_model = load_model\n","\n","    def encode_data(self, X_train, X_test, X_val, y_train, y_test, y_val):\n","\n","        # Applying text encoding.\n","        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        train_encodings = tokenizer(X_train, max_length=self.max_length,\n","                                    return_tensors=\"pt\",  padding=self.padding, truncation=self.truncation)\n","        val_encodings = tokenizer(X_val, max_length=self.max_length,\n","                                  return_tensors=\"pt\",  padding=self.padding, truncation=self.truncation)\n","        test_encodings = tokenizer(X_test, max_length=self.max_length,\n","                                   return_tensors=\"pt\",  padding=self.padding, truncation=self.truncation)\n","\n","        # Formating dataset on Pytorch manners.\n","        train_dataset = CustomDataset(train_encodings, y_train)\n","        val_dataset = CustomDataset(val_encodings, y_val)\n","        test_dataset = CustomDataset(test_encodings, y_test)\n","\n","        # Splitting data in batches.\n","        train_dataloader = DataLoader(\n","            train_dataset, shuffle=True, batch_size=self.batch_size, worker_init_fn=self.seed)\n","        eval_dataloader = DataLoader(\n","            val_dataset, batch_size=self.batch_size, worker_init_fn=self.seed)\n","        test_dataloader = DataLoader(\n","            test_dataset, batch_size=self.batch_size, worker_init_fn=self.seed)\n","\n","        return train_dataloader, eval_dataloader, test_dataloader\n","\n","    def fit_predict(self, X_train, X_test, X_val, y_train, y_test, y_val, num_labels):\n","\n","        # Preparing data.\n","        train_dataloader, eval_dataloader, test_dataloader = self.encode_data(\n","            X_train, X_test, X_val, y_train, y_test, y_val)\n","        \n","        print(\"Loading pre trained model...\")\n","        model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=num_labels)\n","        print(\"Model already in memory.\")\n","        \n","        optimizer = AdamW(model.parameters(), lr=self.lr)\n","        num_training_steps = self.max_epochs * len(train_dataloader)\n","        lr_scheduler = get_scheduler(\n","            \"linear\",\n","            optimizer=optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps=num_training_steps\n","        )\n","\n","        # Sending model to GPU.\n","        device = torch.device(\n","            \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","        model.to(device)\n","\n","        progress_bar = tqdm(range(num_training_steps))\n","        cont_patient = 0\n","        min_loss_eval = 10000\n","\n","        # For each epoch.\n","        for epoch in range(self.max_epochs):\n","            \n","            # ---------- TRAIN ---------- #\n","            model.train()\n","            # For each batch.\n","            for batch in train_dataloader:\n","                progress_bar.update(1)\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                outputs = model(**batch)\n","                loss = outputs.loss\n","                loss.backward()\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad()\n","\n","            # ---------- VALIDATION ---------- #\n","            y_pred_list = []\n","            y_true_list = []\n","            eval_logits = []\n","            model.eval()\n","            for batch in eval_dataloader:\n","\n","                batch = {k: v.to(device) for k, v in batch.items()}\n","                \n","                with torch.no_grad():\n","                    outputs = model(**batch)\n","                \n","                # Saving validation logits.\n","                eval_logits.append(outputs.logits.cpu().numpy().tolist())\n","                \n","                loss = outputs.loss\n","                \n","                predictions = torch.argmax(outputs.logits, dim=-1)\n","                y_pred_list.append(predictions.tolist())\n","                y_true_list.append(list(batch[\"labels\"].tolist()))\n","\n","            y_pred_batch = []\n","            y_true_batch = []\n","\n","            for y_batch in y_pred_list:  # y_batchs\n","                for y_doc in y_batch:\n","                    y_pred_batch.append(y_doc)\n","\n","            for y_batch in y_true_list:  # y_batchs\n","                for y_doc in y_batch:\n","                    y_true_batch.append(y_doc)\n","\n","            loss_eval_atual = loss.item()\n","\n","            # Stop training if no improviment was showed.\n","            if loss_eval_atual < min_loss_eval:\n","                cont_patient = 0\n","                min_loss_eval = loss_eval_atual\n","            else:\n","                cont_patient += 1\n","\n","            if cont_patient >= self.limit_patient:\n","                break\n","\n","        # ---------- TESTE ---------- #\n","        test_probs = []\n","        test_logits = []\n","        model.eval()\n","        for batch in test_dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            with torch.no_grad():\n","                outputs = model(**batch)\n","                # Saving test logits.\n","                test_logits.append(outputs.logits.cpu().numpy().tolist())\n","                # Applying softmax on logits and saving probabilities.\n","                norm = softmax(outputs.logits, dim=-1).tolist()\n","                test_probs.append(norm)\n","\n","        test_probs = np.vstack(test_probs)\n","        eval_logits = np.vstack(eval_logits)\n","        test_logits = np.vstack(test_logits)\n","        return test_probs, test_logits, eval_logits\n","\n","def get_train_probas(X, y, base_path, num_labels, n_splits=4):\n","\n","    # Spliitng train probabilities.\n","    sfk = StratifiedKFold(n_splits=n_splits)\n","    sfk.get_n_splits(X, y)\n","    # This list will hold all the folds probabilities.\n","    probas = []\n","    # This list witl hold all the document's indexes to re sort later.\n","    # When fine-tuning is done.\n","    idx_list = []\n","    alig_idx = np.arange(y.shape[0])\n","    # For each fold.\n","    for fold, (train_index, test_index) in enumerate(sfk.split(X, y)):\n","                \n","        # Selecting train documents and labels.\n","        X_train = get_doc_by_id(X, train_index)\n","        y_train = y[train_index]\n","        \n","        # Spliting the train documents in train and validation.\n","        X_train, X_val, y_train, y_val = train_test_split(\n","            X_train, y_train, test_size=0.1)\n","        \n","        # Getting test documents.\n","        X_test = get_doc_by_id(X, test_index)\n","        y_test = y[test_index]\n","        align = alig_idx[test_index]\n","        \n","        idx_list.append(align)\n","\n","        # Applying oversampling when it is needed. Sometimes a class doesn't have\n","        # enough documents from a class.\n","        for c in set(y_test) - set(y_train):\n","\n","            sintetic = \"fake document\"\n","            X_train.append(sintetic)\n","            y_train = np.hstack([y_train, [c]])\n","        \n","        # Loading the pre-trained model into the memory.\n","        model = Transformer()\n","        # Training and predicing.\n","        test_p, test_l, eval_l = model.fit_predict(\n","            X_train, X_test, X_val, y_train, y_test, y_val, num_labels)\n","        \n","        # Saving train and validation logits.\n","        subfold_path = f\"{base_path}/sub_fold/{fold}\"\n","        os.makedirs(subfold_path, exist_ok=True)\n","        np.savez(f\"{subfold_path}/eval_logits\", X_eval=eval_l, y_eval=y_val)\n","        np.savez(f\"{subfold_path}/test_logits\", X_test=test_l, y_test=y_test)\n","        \n","        # Saving fold document's indexes.\n","        np.savez(f\"{subfold_path}/align\", align=align)\n","        \n","        # Saving fold's probabilities.\n","        probas.append(test_p)\n","        scoring = {}\n","        \n","        # Printing model's performance.\n","        y_pred = test_p.argmax(axis=1)\n","        scoring[\"macro\"] = f1_score(y_test, y_pred, average=\"macro\")\n","        scoring[\"micro\"] = f1_score(y_test, y_pred, average=\"micro\")\n","        print(\n","            f\"\\t\\tSUB-FOLD {fold} - Macro: {scoring['macro']} - Micro {scoring['micro']}\")\n","        torch.cuda.empty_cache()\n","    \n","    # Joining folds probabilities.\n","    probas = np.vstack(probas)\n","    \n","    # Resorting document's probabilities.\n","    sorted_idxs = np.hstack(idx_list).argsort()\n","    probas = probas[sorted_idxs]\n","    probas_path = f\"{base_path}/train\"\n","    # Saving document's proabilities.\n","    np.savez(probas_path, X_train=probas)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-16T19:21:13.421234Z","iopub.status.busy":"2023-02-16T19:21:13.420778Z","iopub.status.idle":"2023-02-16T19:22:39.667002Z","shell.execute_reply":"2023-02-16T19:22:39.664499Z","shell.execute_reply.started":"2023-02-16T19:21:13.421201Z"},"trusted":true},"outputs":[],"source":["SEED = 42\n","\n","home_dir = \"/home/welton/data\"\n","data_source = f\"/home/welton/data/datasets\"\n","datasets = [\"20ng\"]\n","classifier_name = \"bert-base-cased\"\n","classifier_short_name = \"bert\"\n","train_n_splits = 4\n","\n","# For each dataset.\n","for dataset in datasets:\n","    for fold in np.arange(10):\n","        \n","        from transformers import logging\n","        logging.set_verbosity_error()        \n","        \n","        # Setting static seed to prevent variational results.\n","        random.seed(SEED)\n","        torch.manual_seed(SEED)\n","        np.random.seed(seed=SEED)\n","        \n","        print(f\"{dataset.upper()} - FOLD: {fold}\")\n","\n","        # Splitting data.\n","        data_processed = data_process(data_source, dataset, fold)\n","        X_train, X_test, X_val, y_train, y_test, y_val, sp_settings, num_labels = data_processed\n","        \n","        base_path = f\"{home_dir}/clfs_output/split_10/{dataset}/10_folds/{classifier_short_name}/{fold}\"\n","        \n","        os.makedirs(base_path, exist_ok=True)\n","        test_path = f\"{base_path}/test\"\n","        eval_path = f\"{base_path}/eval\"\n","        eval_logits_path = f\"{base_path}/eval_logits\"\n","        test_logits_path = f\"{base_path}/test_logits\"\n","\n","        # Se este fold ainda não foi executado.\n","        if not os.path.exists(test_path):\n","            print(\"Builind test probabilities...\")\n","            # Setting model's parameters.\n","            model = Transformer(padding=True, truncation=True, seed=SEED)\n","            \n","            # Training and predicting.\n","            test_p, test_l, eval_l = model.fit_predict(\n","                X_train, X_test, X_val, y_train, y_test, y_val, num_labels)\n","            \n","            # Saving model's probabilities.\n","            np.savez(test_path, X_test=test_p)\n","            # Saving model's logits.\n","            np.savez(eval_logits_path, X_eval=eval_l, y_eval=y_val)\n","            np.savez(test_logits_path, X_test=test_l, y_test=y_test)\n","            # Printing model performance.\n","            y_pred = test_p.argmax(axis=1)\n","            print(f\"Macro: {f1_score(y_test, y_pred, average='macro')}\")\n","            print(f\"Micro: {f1_score(y_test, y_pred, average='micro')}\")\n","            torch.cuda.empty_cache()\n","            \n","        train_path = f\"{base_path}/train\"\n","        # If train probabilities weren't computed yet.\n","        if not os.path.exists(train_path):\n","            print(\"Builind train probabilities...\")\n","            # Joining train indexes with validation indexes.\n","            idxs = sp_settings.iloc[fold][\"train_idxs\"] + sp_settings.iloc[fold][\"val_idxs\"]\n","            # Sorting indexes. It's important to match the train document's probabilities\n","            # of data split with validantion and without validation (just train and test).\n","            sort = np.array(idxs).argsort()\n","            # Computing train probabilities.\n","            get_train_probas(get_doc_by_id(X_train + X_val, sort),\n","                             np.hstack([y_train, y_val])[sort], base_path, num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
